{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a5a8f2",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf32154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1775ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the URL of the webpage you want to access\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3361ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a396e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the header tags (h1 to h6) in the HTML content\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab95d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of header tag text\n",
    "header_text = [tag.text for tag in header_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d6cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Header\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a pandas DataFrame from the header text list\n",
    "df = pd.DataFrame({'Header': header_text})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4808dcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd1b38c",
   "metadata": {},
   "source": [
    "question - 2 : - Write a python program to display IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release)\n",
    "and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a658fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556a3714",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/chart/top\"\n",
    "response = requests.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282d350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "movie_list = soup.select(\"td.titleColumn\")\n",
    "rating_list = soup.select(\"td.ratingColumn.imdbRating\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751a09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "years = []\n",
    "ratings = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd62ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie, rating in zip(movie_list, rating_list):\n",
    "    name = movie.find(\"a\").text\n",
    "    year = movie.find(\"span\").text.strip(\"()\")\n",
    "    rating = rating.text.strip()\n",
    "    names.append(name)\n",
    "    years.append(year)\n",
    "    ratings.append(rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb715b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Name\": names, \"Year\": years, \"Rating\": ratings}\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea868136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Name  Year Rating\n",
      "0  The Shawshank Redemption  1994    9.2\n",
      "1             The Godfather  1972    9.2\n",
      "2           The Dark Knight  2008    9.0\n",
      "3     The Godfather Part II  1974    9.0\n",
      "4              12 Angry Men  1957    9.0\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482af5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4703616",
   "metadata": {},
   "source": [
    "question : - 3 Write a python program to display IMDB’s Top rated 50 Indian movies’ data (i.e. name, rating, year of\n",
    "release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7bda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61bdf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/india/top-rated-indian-movies/\"\n",
    "response = requests.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba126331",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "movie_list = soup.select(\"td.titleColumn\")\n",
    "rating_list = soup.select(\"td.ratingColumn.imdbRating\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c53acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "years = []\n",
    "ratings = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af94b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie, rating in zip(movie_list, rating_list):\n",
    "    name = movie.find(\"a\").text\n",
    "    year = movie.find(\"span\").text.strip(\"()\")\n",
    "    rating = rating.text.strip()\n",
    "    names.append(name)\n",
    "    years.append(year)\n",
    "    ratings.append(rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37da700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Name\": names, \"Year\": years, \"Rating\": ratings}\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "706400a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Name  Year Rating\n",
      "0  Ramayana: The Legend of Prince Rama  1993    8.6\n",
      "1           Rocketry: The Nambi Effect  2022    8.4\n",
      "2                              Nayakan  1987    8.4\n",
      "3                             Gol Maal  1979    8.4\n",
      "4                          777 Charlie  2022    8.4\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4385a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9fda78a",
   "metadata": {},
   "source": [
    "Question : - 4 Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f43db22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found on the page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-president.htm\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Check if the table exists on the page\n",
    "table = soup.find(\"table\", {\"class\": \"tablepress\"})\n",
    "if table:\n",
    "    presidents_rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    for row in presidents_rows:\n",
    "        data = row.find_all(\"td\")\n",
    "        name = data[0].text.strip()\n",
    "        term = data[1].text.strip()\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "\n",
    "    data = {\"Name\": names, \"Term of Office\": terms}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Table not found on the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c991da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6faddeec",
   "metadata": {},
   "source": [
    "Question :- 5 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "    \n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "c) Top 10 ODI bowlers along with the records of their team andrating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0b1d085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams:\n",
      "           Team Matches Points Rating\n",
      "0   New Zealand      31  3,504    113\n",
      "1         India      47  5,294    113\n",
      "2       England      36  3,988    111\n",
      "3      Pakistan      25  2,649    106\n",
      "4  South Africa      31  3,141    101\n",
      "5    Bangladesh      38  3,625     95\n",
      "6     Sri Lanka      36  3,099     86\n",
      "7   West Indies      43  3,105     72\n",
      "8   Afghanistan      20  1,419     71\n",
      "9       Ireland      27  1,384     51\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m batsmen_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batsman \u001b[38;5;129;01min\u001b[39;00m batsmen[:\u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m---> 31\u001b[0m     batsman_name \u001b[38;5;241m=\u001b[39m \u001b[43mbatsman\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtable-body__cell name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     32\u001b[0m     batsman_team \u001b[38;5;241m=\u001b[39m batsman\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable-body__logo-text\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     33\u001b[0m     batsman_rating \u001b[38;5;241m=\u001b[39m batsman\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable-body__cell u-text-right rating\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape top 10 ODI teams\n",
    "url_teams = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response_teams = requests.get(url_teams)\n",
    "soup_teams = BeautifulSoup(response_teams.content, \"html.parser\")\n",
    "\n",
    "teams = soup_teams.find_all(\"tr\", {\"class\": \"table-body\"})\n",
    "teams_data = []\n",
    "for team in teams[:10]:\n",
    "    team_name = team.find(\"span\", {\"class\": \"u-hide-phablet\"}).text\n",
    "    matches = team.find_all(\"td\")[2].text\n",
    "    points = team.find_all(\"td\")[3].text\n",
    "    rating = team.find_all(\"td\")[4].text\n",
    "    teams_data.append([team_name, matches, points, rating])\n",
    "\n",
    "teams_df = pd.DataFrame(teams_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(\"Top 10 ODI teams:\")\n",
    "print(teams_df)\n",
    "\n",
    "# Scrape top 10 ODI batsmen\n",
    "url_batsmen = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response_batsmen = requests.get(url_batsmen)\n",
    "soup_batsmen = BeautifulSoup(response_batsmen.content, \"html.parser\")\n",
    "\n",
    "batsmen = soup_batsmen.find_all(\"tr\", {\"class\": \"table-body\"})\n",
    "batsmen_data = []\n",
    "for batsman in batsmen[:10]:\n",
    "    batsman_name = batsman.find(\"td\", {\"class\": \"table-body__cell name\"}).text.strip()\n",
    "    batsman_team = batsman.find(\"span\", {\"class\": \"table-body__logo-text\"}).text.strip()\n",
    "    batsman_rating = batsman.find(\"td\", {\"class\": \"table-body__cell u-text-right rating\"}).text.strip()\n",
    "    batsmen_data.append([batsman_name, batsman_team, batsman_rating])\n",
    "\n",
    "batsmen_df = pd.DataFrame(batsmen_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(\"\\nTop 10 ODI batsmen:\")\n",
    "print(batsmen_df)\n",
    "\n",
    "# Scrape top 10 ODI bowlers\n",
    "url_bowlers = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response_bowlers = requests.get(url_bowlers)\n",
    "soup_bowlers = BeautifulSoup(response_bowlers.content, \"html.parser\")\n",
    "\n",
    "bowlers = soup_bowlers.find_all(\"tr\", {\"class\": \"table-body\"})\n",
    "bowlers_data = []\n",
    "for bowler in bowlers[:10]:\n",
    "    bowler_name = bowler.find(\"td\", {\"class\": \"table-body__cell name\"}).text.strip()\n",
    "    bowler_team = bowler.find(\"span\", {\"class\": \"table-body__logo-text\"}).text.strip()\n",
    "    bowler_rating = bowler.find(\"td\", {\"class\": \"table-body__cell u-text-right rating\"}).text.strip()\n",
    "    bowlers_data.append([bowler_name, bowler_team, bowler_rating])\n",
    "\n",
    "bowlers_df = pd.DataFrame(bowlers_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(\"\\nTop 10 ODI bowlers:\")\n",
    "print(bowlers_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31aea0",
   "metadata": {},
   "source": [
    "question :- 6 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7043cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women's cricket:\n",
      "  Position              Team Matches Points Rating\n",
      "0        1    Australia\\nAUS      21  3,603    172\n",
      "1        2      England\\nENG      28  3,342    119\n",
      "2        3  South Africa\\nSA      26  3,098    119\n",
      "3        4        India\\nIND      27  2,820    104\n",
      "4        5   New Zealand\\nNZ      25  2,553    102\n",
      "5        6   West Indies\\nWI      27  2,535     94\n",
      "6        7   Bangladesh\\nBAN      13    983     76\n",
      "7        8     Thailand\\nTHA      11    821     75\n",
      "8        9     Pakistan\\nPAK      27  1,678     62\n",
      "9       10     Sri Lanka\\nSL       8    353     44\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m table_batting \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m data_batting \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable_batting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtbody\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     37\u001b[0m     cols \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [col\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Top 10 ODI teams in women's cricket\n",
    "url_teams = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response = requests.get(url_teams)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table_teams = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "data_teams = []\n",
    "for row in table_teams.tbody.find_all(\"tr\"):\n",
    "    cols = row.find_all(\"td\")\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data_teams.append(cols)\n",
    "\n",
    "headers_teams = [\n",
    "    \"Position\",\n",
    "    \"Team\",\n",
    "    \"Matches\",\n",
    "    \"Points\",\n",
    "    \"Rating\",\n",
    "]\n",
    "\n",
    "df_teams = pd.DataFrame(data_teams, columns=headers_teams)[:10]\n",
    "print(\"Top 10 ODI teams in women's cricket:\")\n",
    "print(df_teams)\n",
    "\n",
    "# Top 10 women's ODI batting players\n",
    "url_batting = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url_batting)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table_batting = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "data_batting = []\n",
    "for row in table_batting.tbody.find_all(\"tr\"):\n",
    "    cols = row.find_all(\"td\")\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data_batting.append(cols)\n",
    "\n",
    "headers_batting = [\n",
    "    \"Position\",\n",
    "    \"Player\",\n",
    "    \"Team\",\n",
    "    \"Rating\",\n",
    "]\n",
    "\n",
    "df_batting = pd.DataFrame(data_batting, columns=headers_batting)[:10]\n",
    "print(\"\\nTop 10 women's ODI batting players:\")\n",
    "print(df_batting)\n",
    "\n",
    "# Top 10 women's ODI all-rounders\n",
    "url_allrounders = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response = requests.get(url_allrounders)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table_allrounders = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "data_allrounders = []\n",
    "for row in table_allrounders.tbody.find_all(\"tr\"):\n",
    "    cols = row.find_all(\"td\")\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data_allrounders.append(cols)\n",
    "\n",
    "headers_allrounders = [\n",
    "    \"Position\",\n",
    "    \"Player\",\n",
    "    \"Team\",\n",
    "    \"Rating\",\n",
    "]\n",
    "\n",
    "df_allrounders = pd.DataFrame(data_allrounders, columns=headers_allrounders)[:10]\n",
    "print(\"\\nTop 10 women's ODI all-rounders:\")\n",
    "print(df_allrounders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119df3c",
   "metadata": {},
   "source": [
    "question :- 7 Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data framei) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "108538ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL to be scraped\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# send a request to the website and get the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# create a soup object for parsing the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# find all the articles\n",
    "articles = soup.find_all('li', {'class': 'result-list-item'})\n",
    "\n",
    "# create a list of details for each article\n",
    "data = []\n",
    "for article in articles:\n",
    "    title = article.find('a', {'class': 'js-article-title'})['title']\n",
    "    authors = ', '.join([author.text.strip() for author in article.find_all('span', {'class': 'text-xs'})])\n",
    "    published_date = article.find('span', {'class': 'text-xs journal-text-underline'}).text.strip()\n",
    "    paper_url = \"https://www.journals.elsevier.com\" + article.find('a', {'class': 'js-article-title'})['href']\n",
    "    data.append([title, authors, published_date, paper_url])\n",
    "\n",
    "# create a pandas dataframe from the data\n",
    "df = pd.DataFrame(data, columns=['Paper Title', 'Authors', 'Published Date', 'Paper URL'])\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201cd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "831a4684",
   "metadata": {},
   "source": [
    "question :- 8 Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71444809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL to be scraped\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# send a request to the website and get the HTML content\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# create a soup object for parsing the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# find all the paper details\n",
    "paper_details = soup.find_all('div', {'class': 'pod-listing-header'})\n",
    "\n",
    "# create lists to store paper title, authors, published date and paper URL\n",
    "paper_titles = []\n",
    "paper_authors = []\n",
    "paper_dates = []\n",
    "paper_urls = []\n",
    "\n",
    "# loop through each paper detail and extract the required information\n",
    "for detail in paper_details:\n",
    "    # extract paper title and URL\n",
    "    paper_title = detail.find('a', {'class': 'pod-listing-header__link'}).text.strip()\n",
    "    paper_url = detail.find('a', {'class': 'pod-listing-header__link'})['href']\n",
    "    paper_titles.append(paper_title)\n",
    "    paper_urls.append(paper_url)\n",
    "\n",
    "    # extract authors\n",
    "    authors = detail.find('ul', {'class': 'pod-listing-header__authors'}).find_all('li')\n",
    "    author_list = []\n",
    "    for author in authors:\n",
    "        author_list.append(author.text.strip())\n",
    "    paper_authors.append(\", \".join(author_list))\n",
    "\n",
    "    # extract published date\n",
    "    published_date = detail.find('div', {'class': 'pod-listing-header__details'}).find('span').text.strip()\n",
    "    paper_dates.append(published_date)\n",
    "\n",
    "# create a dictionary of data\n",
    "data = {\n",
    "    'Paper Title': paper_titles,\n",
    "    'Authors': paper_authors,\n",
    "    'Published Date': paper_dates,\n",
    "    'Paper URL': paper_urls\n",
    "}\n",
    "\n",
    "# create a pandas dataframe from the data dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf0d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b568986b",
   "metadata": {},
   "source": [
    "question 9 : - Write a python program to scrape mentioned details from dineout.co.in and make data frame\n",
    "    \n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bdd52794",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     restaurants\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 26\u001b[0m cuisine \u001b[38;5;241m=\u001b[39m \u001b[43mcard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrestnt-info cursor-pointer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cuisine:\n\u001b[0;32m     28\u001b[0m     cuisines\u001b[38;5;241m.\u001b[39mappend(cuisine\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'p'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a request to the website\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract required information from the webpage\n",
    "restaurants = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "for card in soup.find_all('div', class_='restnt-card'):\n",
    "    name = card.find('div', class_='restnt-info cursor-pointer')\n",
    "    if name:\n",
    "        restaurants.append(name.h5.text.strip())\n",
    "    else:\n",
    "        restaurants.append(None)\n",
    "        \n",
    "    cuisine = card.find('div', class_='restnt-info cursor-pointer').p\n",
    "    if cuisine:\n",
    "        cuisines.append(cuisine.text.strip())\n",
    "    else:\n",
    "        cuisines.append(None)\n",
    "        \n",
    "    location = card.find('div', class_='restnt-info cursor-pointer').find_all('p')[1]\n",
    "    if location:\n",
    "        locations.append(location.text.strip())\n",
    "    else:\n",
    "        locations.append(None)\n",
    "    \n",
    "    rating = card.find('div', class_='restnt-info cursor-pointer').find('div', class_='restnt-rating')\n",
    "    if rating:\n",
    "        ratings.append(rating.text.strip())\n",
    "    else:\n",
    "        ratings.append(None)\n",
    "    \n",
    "    image_url = card.find('div', class_='restnt-img').find('img')\n",
    "    if image_url:\n",
    "        image_urls.append(image_url['data-src'])\n",
    "    else:\n",
    "        image_urls.append(None)\n",
    "\n",
    "# Create a pandas dataframe with the extracted information\n",
    "df = pd.DataFrame({\n",
    "    'Restaurant Name': restaurants,\n",
    "    'Cuisine': cuisines,\n",
    "    'Location': locations,\n",
    "    'Ratings': ratings,\n",
    "    'Image URL': image_urls\n",
    "})\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
